{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 13:45:00.720023: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-04 13:45:00.816201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/guillaume/anaconda3/envs/semproj/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.7/lib64:\n",
      "2023-06-04 13:45:00.816218: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-04 13:45:01.388225: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/guillaume/anaconda3/envs/semproj/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.7/lib64:\n",
      "2023-06-04 13:45:01.388284: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/guillaume/anaconda3/envs/semproj/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.7/lib64:\n",
      "2023-06-04 13:45:01.388290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import tensorflow as tf\n",
    "# assert tf.__version__.startswith('2')\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hand_coordinates(pose_landmarks):\n",
    "    left_hand_coords = {\"max_x\": -1000, \"max_y\": -1000, \"min_x\": 1000, \"min_y\": 1000}\n",
    "    right_hand_coords = {\"max_x\": -1000, \"max_y\": -1000, \"min_x\": 1000, \"min_y\": 1000}\n",
    "\n",
    "    left_hand_index = [17, 19, 21] # pinky index thumb\n",
    "    right_hand_index = [18, 20, 22]\n",
    "\n",
    "    # Extract the coordinates of the left and right hands\n",
    "    for idx, landmark in enumerate(pose_landmarks.landmark):\n",
    "        if idx in left_hand_index:\n",
    "              # Left hand\n",
    "            if landmark.x > left_hand_coords[\"max_x\"]:\n",
    "                left_hand_coords[\"max_x\"] = landmark.x\n",
    "            if landmark.y > left_hand_coords[\"max_y\"]:\n",
    "                left_hand_coords[\"max_y\"] = landmark.y\n",
    "            if landmark.x < left_hand_coords[\"min_x\"]:\n",
    "                left_hand_coords[\"min_x\"] = landmark.x\n",
    "            if landmark.y < left_hand_coords[\"min_y\"]:\n",
    "                left_hand_coords[\"min_y\"] = landmark.y\n",
    "\n",
    "        elif idx in right_hand_index:  # Right hand\n",
    "\n",
    "            if landmark.x > right_hand_coords[\"max_x\"]:\n",
    "                right_hand_coords[\"max_x\"] = landmark.x\n",
    "            if landmark.y > right_hand_coords[\"max_y\"]:\n",
    "                right_hand_coords[\"max_y\"] = landmark.y\n",
    "            if landmark.x < right_hand_coords[\"min_x\"]:\n",
    "                right_hand_coords[\"min_x\"] = landmark.x\n",
    "            if landmark.y < right_hand_coords[\"min_y\"]:\n",
    "                right_hand_coords[\"min_y\"] = landmark.y\n",
    "\n",
    "\n",
    "    if any(item < 0 or item > 1 for item in left_hand_coords.values()): # coordinates are out of the frame\n",
    "        left_hand_coords = None\n",
    "    if any(item <0 or item > 1 for item in right_hand_coords.values()):\n",
    "        right_hand_coords = None\n",
    "\n",
    "    return right_hand_coords, left_hand_coords # we have to switch them here due to the cv2.flip\n",
    "\n",
    "def crop_hand(frame, hand_coords):\n",
    "    # image_height, image_width = frame.height, frame.width\n",
    "    \n",
    "    image_height, image_width, _ = frame.shape\n",
    "\n",
    "    margin = 80\n",
    "\n",
    "    x_min = max(0, int(hand_coords[\"min_x\"] * image_width) - margin)\n",
    "    y_min = max(0, int(hand_coords[\"min_y\"] * image_height) - margin)\n",
    "    x_max = min(image_width - 1, int(hand_coords[\"max_x\"] * image_width) + margin)\n",
    "    y_max = min(image_height - 1, int(hand_coords[\"max_y\"] * image_height) + margin)\n",
    "\n",
    "    # Crop the image around the hand region\n",
    "    hand_image = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    return hand_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_hand(mp_image):\n",
    "\n",
    "        # mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data = hand_image)\n",
    "        # mp_image = hand_image\n",
    "        result = recognizer.recognize(mp_image) \n",
    "    \n",
    "        try:\n",
    "            prediction = result.gestures[0][0].category_name\n",
    "            confidence = result.gestures[0][0].score          \n",
    "        except:\n",
    "            prediction = \"no hands hee\"\n",
    "            confidence = \"\"\n",
    "\n",
    "        return prediction, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "found_rgb = False\n",
    "\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "\n",
    "# we enable rgb stream \n",
    "if device_product_line == 'L500':\n",
    "    config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)\n",
    "else:\n",
    "    config.enable_stream(rs.stream.color, 1280, 800, rs.format.bgr8, 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W20230604 13:56:36.439347 11229 gesture_recognizer_graph.cc:128] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n"
     ]
    }
   ],
   "source": [
    "pipeline.start(config)\n",
    "\n",
    "new_frame_time = 0\n",
    "prev_frame_time = 0\n",
    "\n",
    "cv2.namedWindow('Webcam Feed', cv2.WINDOW_NORMAL)\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a gesture recognizer instance with the image mode:\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='exported_model_3/gesture_recognizer.task'),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_hands = 1,\n",
    "    min_hand_detection_confidence=0.2,\n",
    "    min_hand_presence_confidence=0.2)\n",
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "    with mp_pose.Pose(\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as pose:    \n",
    "        while True:\n",
    "\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            color_frame = frames.get_color_frame()\n",
    "            \n",
    "            if not color_frame:\n",
    "                continue\n",
    "\n",
    "            color_image = np.asarray(color_frame.get_data())\n",
    "            frame = cv2.flip(color_image,1)\n",
    "\n",
    "            #hands detection and classification\n",
    "            pose_results = pose.process(frame) # find body keypoints\n",
    "\n",
    "            right_pred = \"no hands\"\n",
    "            right_conf = \"\"\n",
    "\n",
    "            if pose_results.pose_landmarks:\n",
    "                # Get the coordinates of the left and right hands\n",
    "                _, right_hand_coords = get_hand_coordinates(pose_results.pose_landmarks)\n",
    "                \n",
    "                if right_hand_coords is not None:\n",
    "                    right_hand_image = (crop_hand(frame, right_hand_coords))\n",
    "                    # right_hand_image = cv2.flip(right_hand_image,1)\n",
    "                    # right_hand_image = cv2.flip(right_hand_image,1)\n",
    "                    right_hand_image = cv2.cvtColor(right_hand_image, cv2.COLOR_BGR2RGB)\n",
    "                    right_hand_image = mp.Image(image_format=mp.ImageFormat.SRGB, data = right_hand_image)\n",
    "                    right_pred, right_conf = classify_hand(right_hand_image)\n",
    "                    \n",
    "\n",
    "                    # Draw the bounding box and prediction on the frame\n",
    "                    margin_x = 80\n",
    "                    margin_y = int(margin_x * 1.25)\n",
    "                    image_height, image_width, _ = frame.shape\n",
    "                    x_min = max(0, int(right_hand_coords[\"min_x\"] * image_width) - margin_x)\n",
    "                    y_min = max(0, int(right_hand_coords[\"min_y\"] * image_height) - margin_y)\n",
    "                    x_max = min(image_width - 1, int(right_hand_coords[\"max_x\"] * image_width) + margin_x)\n",
    "                    y_max = min(image_height - 1, int(right_hand_coords[\"max_y\"] * image_height) + margin_y)\n",
    "                    start_box = (x_min, y_min)\n",
    "                    end_box = (x_max, y_max)\n",
    "                    frame = cv2.rectangle(frame, start_box, end_box, (255,0,0), 2)\n",
    "                    cv2.putText(frame, right_pred, start_box, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "            # annotated_image = draw_landmarks_on_image(frame, result)\n",
    "            annotated_image = frame\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            pose_results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "            new_frame_time = time.time()\n",
    "            fps = 1/(new_frame_time-prev_frame_time)\n",
    "            prev_frame_time = new_frame_time\n",
    "\n",
    "            # Write fps on the frame\n",
    "            cv2.putText(annotated_image, f\"FPS: {fps}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Display the frame in a window named 'Webcam Feed'\n",
    "            cv2.imshow('Webcam Feed', annotated_image)\n",
    "            \n",
    "            # Exit the loop if the 'q' key is pressed\n",
    "            if cv2.waitKey(1) == 27:  # 27 is the ASCII code for the Escape key\n",
    "                break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
