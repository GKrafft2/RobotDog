{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = \"data/train/\"\n",
    "DATA_TEST_PATH = \"data/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data( name, data_path):\n",
    "# label = 0: idle 1: roll_droite, 2: roll_gauche, 3: salut_droite, 4: salut_gauche (determinÃ© apar l'ordre des pkl dans le dossier data)\n",
    "    master_df = pd.DataFrame(columns = [\"frame\",\"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\", \"label\"])\n",
    "    label = 0\n",
    "    for pickle_file in os.listdir(data_path):\n",
    "        if pickle_file.endswith(name+\".pkl\"):\n",
    "            temp_df = pd.read_pickle(data_path + pickle_file)\n",
    "            temp_df[\"label\"] = label\n",
    "            label += 1\n",
    "            master_df = pd.concat([master_df, temp_df], axis = 0)\n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_path):\n",
    "    master_df = pd.concat([get_data(\"gui\", data_path), get_data(\"val\", data_path)], axis = 0)\n",
    "    temp_df = master_df[[\"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\"]]\n",
    "    # min max normalization per angle\n",
    "    # master_df[[\"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\"]] = (temp_df-temp_df.min())/(temp_df.max()-temp_df.min()\n",
    "    master_df[[\"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\"]] = ((temp_df-90)/180)\n",
    "    # drop nan to avoid instability\n",
    "    master_df = master_df.dropna()\n",
    "\n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, window_size):\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        n_vids = 8 #number of videos - we cannot take frames that are not in the same video\n",
    "        return len(self.df) - self.window_size*n_vids + 2*n_vids\n",
    "\n",
    "    def __getitem__(self, idx): #for now we take window_size consecutive frames\n",
    "        labels = self.df.iloc[idx:idx+self.window_size, 5].values\n",
    "        same = True\n",
    "        for label in labels:\n",
    "            if label != labels[0]:\n",
    "                same = False\n",
    "        if same == False:\n",
    "            return self.__getitem__(idx+1) #if the labels are not the same, we skip this window\n",
    "        else:\n",
    "            label = labels[0]\n",
    "        \n",
    "        data = self.df.iloc[idx:idx+self.window_size, 1:5].values\n",
    "        # data = (data-data.mean(axis=0))/data.std(axis=0)\n",
    "        return torch.tensor(data).float(), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "window_size = 5\n",
    "\n",
    "df_train  = create_dataset(DATA_TRAIN_PATH)\n",
    "df_test = create_dataset(DATA_TEST_PATH)\n",
    "\n",
    "dataset_train = TimeSeriesDataset(df_train, window_size)\n",
    "dataset_test = TimeSeriesDataset(df_test, window_size)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.batch_norm = nn.BatchNorm1d(input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # = self.batch_norm(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = torch.nn.functional.softmax(self.fc(out[:, -1, :]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    print(\"Testing model\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = outputs.argmax(dim = -1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = 100 * correct / total\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliot\\AppData\\Local\\Temp\\ipykernel_12028\\3067478705.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = torch.nn.functional.softmax(self.fc(out[:, -1, :]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "Training => Loss: 1.3793 | Train Accuracy: 0.5373| Test Accuracy: 58.5280\n",
      "Epoch 2/25\n",
      "Testing model\n",
      "Training => Loss: 1.2148 | Train Accuracy: 0.7085| Test Accuracy: 57.3147\n",
      "Epoch 3/25\n",
      "Testing model\n",
      "Training => Loss: 1.1865 | Train Accuracy: 0.7299| Test Accuracy: 59.3570\n",
      "Epoch 4/25\n",
      "Testing model\n",
      "Training => Loss: 1.1763 | Train Accuracy: 0.7323| Test Accuracy: 59.0436\n",
      "Epoch 5/25\n",
      "Testing model\n",
      "Training => Loss: 1.1647 | Train Accuracy: 0.7424| Test Accuracy: 55.2118\n",
      "Epoch 6/25\n",
      "Testing model\n",
      "Training => Loss: 1.1610 | Train Accuracy: 0.7430| Test Accuracy: 56.4351\n",
      "Epoch 7/25\n",
      "Testing model\n",
      "Training => Loss: 1.1544 | Train Accuracy: 0.7517| Test Accuracy: 54.7670\n",
      "Epoch 8/25\n",
      "Testing model\n",
      "Training => Loss: 1.1535 | Train Accuracy: 0.7498| Test Accuracy: 57.0519\n",
      "Epoch 9/25\n",
      "Testing model\n",
      "Training => Loss: 1.1487 | Train Accuracy: 0.7546| Test Accuracy: 57.0721\n",
      "Epoch 10/25\n",
      "Testing model\n",
      "Training => Loss: 1.1445 | Train Accuracy: 0.7575| Test Accuracy: 57.8101\n",
      "Epoch 11/25\n",
      "Testing model\n",
      "Training => Loss: 1.1425 | Train Accuracy: 0.7621| Test Accuracy: 57.0316\n",
      "Epoch 12/25\n",
      "Testing model\n",
      "Training => Loss: 1.1407 | Train Accuracy: 0.7629| Test Accuracy: 58.2348\n",
      "Epoch 13/25\n",
      "Testing model\n",
      "Training => Loss: 1.1373 | Train Accuracy: 0.7658| Test Accuracy: 57.1530\n",
      "Epoch 14/25\n",
      "Testing model\n",
      "Training => Loss: 1.1332 | Train Accuracy: 0.7704| Test Accuracy: 58.0831\n",
      "Epoch 15/25\n",
      "Testing model\n",
      "Training => Loss: 1.1302 | Train Accuracy: 0.7750| Test Accuracy: 57.7495\n",
      "Epoch 16/25\n",
      "Testing model\n",
      "Training => Loss: 1.1305 | Train Accuracy: 0.7724| Test Accuracy: 56.1116\n",
      "Epoch 17/25\n",
      "Testing model\n",
      "Training => Loss: 1.1258 | Train Accuracy: 0.7781| Test Accuracy: 56.4857\n",
      "Epoch 18/25\n",
      "Testing model\n",
      "Training => Loss: 1.1217 | Train Accuracy: 0.7817| Test Accuracy: 57.6281\n",
      "Epoch 19/25\n",
      "Testing model\n",
      "Training => Loss: 1.1192 | Train Accuracy: 0.7841| Test Accuracy: 58.5280\n",
      "Epoch 20/25\n",
      "Testing model\n",
      "Training => Loss: 1.1149 | Train Accuracy: 0.7877| Test Accuracy: 58.2853\n",
      "Epoch 21/25\n",
      "Testing model\n",
      "Training => Loss: 1.1134 | Train Accuracy: 0.7900| Test Accuracy: 56.7587\n",
      "Epoch 22/25\n",
      "Testing model\n",
      "Training => Loss: 1.1112 | Train Accuracy: 0.7952| Test Accuracy: 57.0519\n",
      "Epoch 23/25\n",
      "Testing model\n",
      "Training => Loss: 1.1081 | Train Accuracy: 0.7962| Test Accuracy: 58.2449\n",
      "Epoch 24/25\n",
      "Testing model\n",
      "Training => Loss: 1.1033 | Train Accuracy: 0.8017| Test Accuracy: 58.6594\n",
      "Epoch 25/25\n",
      "Testing model\n",
      "Training => Loss: 1.1033 | Train Accuracy: 0.8019| Test Accuracy: 58.8919\n"
     ]
    }
   ],
   "source": [
    "input_size = 4 # Size of each time step in the input window\n",
    "hidden_size = 64 # Number of features in the hidden state of the LSTM\n",
    "num_layers = 2 # Number of LSTM layers\n",
    "num_classes = 5 # Number of output classes (i.e. number of possible labels)\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 25\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    num_correct = 0\n",
    "    running_epoch_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred = outputs.argmax(dim = -1)\n",
    "\n",
    "        num_correct += (y_pred == labels).sum().item()\n",
    "        running_epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_epoch_loss / len(train_dataloader)\n",
    "    epoch_acc = num_correct / len(train_dataloader.dataset)\n",
    "    test_accuracy = test(model, test_dataloader)\n",
    "    print(f'Training => Loss: {epoch_loss:.4f} | Train Accuracy: {epoch_acc:.4f}| Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = \"pytorch_weights/LSTM/\"\n",
    "torch.save(model.state_dict(), WEIGHTS_PATH + \"lstm_model_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 4])\n",
      "tensor([4, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3975, 0.1500, 0.5010, 0.9925],\n",
       "        [0.3988, 0.1477, 0.4307, 0.9963],\n",
       "        [0.3920, 0.1464, 0.3441, 0.9975],\n",
       "        [0.3980, 0.1483, 0.2525, 0.9982],\n",
       "        [0.3989, 0.1478, 0.2173, 0.9945]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliot\\AppData\\Local\\Temp\\ipykernel_17560\\3067478705.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = torch.nn.functional.softmax(self.fc(out[:, -1, :]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x[0,:,:].unsqueeze(0).to(device)).argmax(dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 2, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:16:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a1eca162d8244ec663334057c1610a3bae01391a28d85af84dac413dee83203"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
