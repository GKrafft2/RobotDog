{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time \n",
    "import pyrealsense2 as rs\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {\"nose\":0,\n",
    "\"left eye (inner)\":1,\n",
    "\"left eye\":2,\n",
    "\"left eye (outer)\":3,\n",
    "\"right eye (inner)\":4,\n",
    "\"right eye\":5,\n",
    "\"right eye (outer)\":6,\n",
    "\"left ear\":7,\n",
    "\"right ear\":8,\n",
    "\"mouth (left)\":9,\n",
    "\"mouth (right)\":10,\n",
    "\"left shoulder\":11,\n",
    "\"right shoulder\":12,\n",
    "\"left elbow\":13,\n",
    "\"right elbow\":14,\n",
    "\"left wrist\":15,\n",
    "\"right wrist\":16,\n",
    "\"left pinky\":17,\n",
    "\"right pinky\":18,\n",
    "\"left index\":19,\n",
    "\"right index\":20,\n",
    "\"left thumb\":21,\n",
    "\"right thumb\":22,\n",
    "\"left hip\":23,\n",
    "\"right hip\":24,\n",
    "\"left knee\":25,\n",
    "\"right knee\":26,\n",
    "\"left ankle\":27,\n",
    "\"right ankle\":28,\n",
    "\"left heel\":29,\n",
    "\"right heel\":30,\n",
    "\"left foot index\":31,\n",
    "\"right foot index\":32}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using mediapipe pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "new_frame_time = 0\n",
    "prev_frame_time = 0\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as pose:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    new_frame_time = time.time()\n",
    "    fps = 1/(new_frame_time-prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image)\n",
    "\n",
    "    # # compute the commands based on the angles of the arms at the shoulder\n",
    "    # left_angle, right_angle = compute_arms_angle(results.pose_landmarks.landmark)\n",
    "    # command = angles_to_command(left_angle, right_angle)\n",
    "    # print(command)\n",
    "\n",
    "    # Draw the pose annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    fps = str(int(fps))\n",
    "    image = cv2.flip(image,1)\n",
    "    cv2.putText(image, fps, (7, 70), 1, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "    cv2.imshow('MediaPipe Pose', image)\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27: # press escape to quit\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using mediapipe Hollistic which finds keypoints for the pose, face landmarks and hand keypoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs at about 15 fps inference is done on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_landmark_to_array(landmarks):\n",
    "    landmarks_array = np.array(\n",
    "        [[landmark.x, landmark.y, landmark.z] for landmark in landmarks])\n",
    "    return landmarks_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    for i in range(3):\n",
    "        channel = data[:,:,i]\n",
    "        mean = np.mean(channel)\n",
    "        std = np.std(channel)\n",
    "        if std != 0: #to avoid dividing by 0 and having nan values in the arrays\n",
    "            data[:,:,i] = (channel - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_frame_time = 0\n",
    "prev_frame_time = 0\n",
    "\n",
    "RUN_NUMBER = 5 # we will save the data in a folder with this number\n",
    "\n",
    "# could find a way of finding this value automatically\n",
    "frames_by_sec = 20 - 1\n",
    "n_frames = -1\n",
    "landmark_image = np.empty((75,3))\n",
    "\n",
    "#hollistic detects posiiton, face and hand landmarks\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "moves = [\"sit\", \"lie down\", \"stay\", \"come\"]\n",
    "\n",
    "n_samples = 30 # number of times we will record each move\n",
    "current_sample = 0 # current sample we are recording\n",
    "current_move = 0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"image\", cv2.WINDOW_NORMAL)\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    if n_frames == frames_by_sec or n_frames == -1: # we make 1 sec recordings -> save the data , reset the counter, pause for 2 seconds and update the sample and move counter\n",
    "\n",
    "        # save and reset the \"image\" array\n",
    "        if n_frames != -1:\n",
    "          landmark_image = np.transpose(landmark_image, (0, 2, 1))\n",
    "          # normalize_data(landmark_image)\n",
    "          np.save(f\"data_test/{moves[current_move]}/{RUN_NUMBER}{current_sample}\", landmark_image)\n",
    "          landmark_image = np.empty((75,3))\n",
    "\n",
    "        n_frames = 0\n",
    "        current_sample += 1\n",
    "        \n",
    "        # display which action will be recorded next\n",
    "        image = cv2.flip(image,1)\n",
    "        cv2.putText(image, f\"prepare for {moves[current_move]} sample no {current_sample}\", (7, 70), fontScale = 2, fontFace = 1, color = (100, 255, 0), thickness = 2, lineType = cv2.LINE_AA)\n",
    "        if current_sample == n_samples:\n",
    "            current_sample = 0\n",
    "            current_move += 1\n",
    "            if current_move == len(moves):\n",
    "               break\n",
    "\n",
    "        cv2.imshow(\"image\", image)\n",
    "        cv2.waitKey(2000)\n",
    "        continue\n",
    "\n",
    "    n_frames += 1\n",
    "\n",
    "    # compute fps\n",
    "    new_frame_time = time.time()\n",
    "    fps = 1/(new_frame_time-prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "\n",
    "    # compute keypoints etc...\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image.flags.writeable = False\n",
    "    results = holistic_model.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # convert landmarks to array with exception handling (zero array if not detected)\n",
    "    try:\n",
    "      pose_landmarks = convert_landmark_to_array(results.pose_landmarks.landmark)\n",
    "    except:\n",
    "      #  print(\"no pose landmarks detected\")\n",
    "       pose_landmarks = np.zeros((33,3))\n",
    "    try:\n",
    "      left_hand_landmarks = convert_landmark_to_array(results.left_hand_landmarks.landmark)\n",
    "    except:\n",
    "      # print(\"no left hand landmarks detected\")\n",
    "      left_hand_landmarks = np.zeros((21,3))\n",
    "    try:\n",
    "      right_hand_landmarks = convert_landmark_to_array(results.right_hand_landmarks.landmark)\n",
    "    except:\n",
    "      # print(\"no right hand landmarks detected\")\n",
    "      right_hand_landmarks = np.zeros((21,3))\n",
    "\n",
    "    hands_and_pose_array = np.concatenate((pose_landmarks, left_hand_landmarks, right_hand_landmarks), axis=0)\n",
    "    landmark_image = np.dstack((landmark_image, hands_and_pose_array))\n",
    "\n",
    "    # Drawing the annotiations\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    # Drawing Pose Landmarks\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    \n",
    "    # Drawing Right hand Landmarks\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image,\n",
    "      results.right_hand_landmarks,\n",
    "      mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    " \n",
    "    # Drawing Left hand Landmarks\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image,\n",
    "      results.left_hand_landmarks,\n",
    "      mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "\n",
    "    fps = str(int(fps))\n",
    "    image = cv2.flip(image,1)\n",
    "    cv2.putText(image, fps, (7, 70), 1, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "    cv2.imshow('image', image)\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27: # press escape to quit\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a1eca162d8244ec663334057c1610a3bae01391a28d85af84dac413dee83203"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
