{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D skeleton tracking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a 3d skeleton using OpenPifPaf to have the coordinates in the image coordinates and use the depth capacities of the intel Realsens d455 to add depth (3rd dimension)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: it's fairly important to use the included usb cable as others cables mignt not be recognized as usb 3.x. This is required in order to have everything working out smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/anaconda3/envs/semproj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import openpifpaf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from timeit import timeit\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "import math as m\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check version and available hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenPifPaf version 0.13.11\n",
      "PyTorch version 1.13.1\n"
     ]
    }
   ],
   "source": [
    "print('OpenPifPaf version', openpifpaf.__version__)\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Openpifpaf predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = openpifpaf.Predictor(checkpoint='shufflenetv2k16') #could use mobilenetv3small for betterperformances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magic numbers and dicts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def compute_angle(shoudler, hip, elbow):\n",
    "    \"\"\" Returns the angle in radiants between of the arm defined by its shoudler, hip and elbow\"\"\"\n",
    "    v1 = unit_vector(hip - shoudler)\n",
    "    v2 = unit_vector(elbow - shoudler)\n",
    "    return np.arccos(np.clip(np.dot(v1, v2),-1.0,1.0))*360/(2*np.pi)\n",
    "\n",
    "  \n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def compute_angle(shoudler, hip, elbow):\n",
    "    \"\"\" Returns the angle in radiants between of the arm defined by its shoudler, hip and elbow\"\"\"\n",
    "    v1 = unit_vector(hip - shoudler)\n",
    "    v2 = unit_vector(elbow - shoudler)\n",
    "    return np.arccos(np.clip(np.dot(v1, v2),-1.0,1.0))*360/(2*np.pi)\n",
    "\n",
    "def compute_arms_angle(prediction):\n",
    "    \"\"\" Returns the angle of left and right arm wrt to the torso (return nan if the arm is not seen) \"\"\"\n",
    "    \n",
    "    pos_dict = {\n",
    "        \"nose\": 0,\n",
    "        \"left_eye\": 1,\n",
    "        \"right_eye\": 2,\n",
    "        \"left_ear\": 3,\n",
    "        \"right_ear\": 4,\n",
    "        \"left_shoudler\": 5,\n",
    "        \"right_shoudler\": 6,\n",
    "        \"left_elbow\": 7,\n",
    "        \"right_elbow\": 8,\n",
    "        \"left_wrist\": 9,\n",
    "        \"right_wrist\": 10,\n",
    "        \"left_hip\": 11,\n",
    "        \"right_hip\": 12,\n",
    "        \"left_knee\": 13,\n",
    "        \"right_knee\": 14,\n",
    "        \"left_ankle\": 15,\n",
    "        \"right_ankle\": 16}\n",
    "    \n",
    "    # we suppose only on person on the image (index = 0)\n",
    "    keypoints = predictions[0].data\n",
    "\n",
    "    # left arm\n",
    "    left_shoulder = keypoints[pos_dict[\"left_shoudler\"],0:2]\n",
    "    left_hip = keypoints[pos_dict[\"left_hip\"],0:2]\n",
    "    left_elbow = keypoints[pos_dict[\"left_elbow\"],0:2]\n",
    "\n",
    "    # right arm\n",
    "    right_shoulder = keypoints[pos_dict[\"right_shoudler\"],0:2]\n",
    "    right_hip = keypoints[pos_dict[\"right_hip\"],0:2]\n",
    "    right_elbow = keypoints[pos_dict[\"right_elbow\"],0:2]\n",
    "\n",
    "    return(compute_angle(left_shoulder, left_hip, left_elbow), compute_angle(right_shoulder, right_hip, right_elbow))\n",
    "\n",
    "def angles_to_command(left_angle, right_angle):\n",
    "    \"\"\"Returns the command corresponding to arms angle\"\"\"\n",
    "    \n",
    "    up_angle = 160\n",
    "    horizontal_angle = 90\n",
    "    down_angle = 20\n",
    "    margin = 15\n",
    "\n",
    "    # backward angle = 160 +/- 10\n",
    "    if (left_angle > up_angle-margin and left_angle < up_angle+margin) and (right_angle > up_angle-margin and right_angle < up_angle+margin):\n",
    "        return \"backward\"\n",
    "    # forward angle = 90 +/- 10\n",
    "    if (left_angle > horizontal_angle-margin and left_angle < horizontal_angle+margin) and (right_angle > horizontal_angle-margin and right_angle < horizontal_angle+margin):\n",
    "        return \"forward\"\n",
    "    # left = left: 90 +/- 10 right: 20 +/- 10\n",
    "    if (left_angle > horizontal_angle-margin and left_angle < horizontal_angle+margin) and (right_angle > down_angle-margin and right_angle < down_angle+margin):\n",
    "        return \"left\"\n",
    "    # right = left: 20 +/- 10 right: 90 +/- 10\n",
    "    if (left_angle > down_angle-margin and left_angle < down_angle+margin) and (right_angle > horizontal_angle-margin and right_angle < horizontal_angle+margin):\n",
    "        return \"right\"\n",
    "    else:\n",
    "        return \"stop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart2sph(x,y,z):\n",
    "    XsqPlusYsq = x**2 + y**2\n",
    "    r = m.sqrt(XsqPlusYsq + z**2)               # r\n",
    "    elev = m.atan2(z,m.sqrt(XsqPlusYsq))     # theta\n",
    "    az = m.atan2(y,x)                           # phi\n",
    "    return r, elev, az\n",
    "\n",
    "def rad2deg(rad):\n",
    "    return rad*180/m.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED = (0, 0, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (255, 0, 0)\n",
    "YELLOW = (0, 255, 255)\n",
    "\n",
    "pos_dict = {\n",
    "        \"nose\": 0,\n",
    "        \"left_eye\": 1,\n",
    "        \"right_eye\": 2,\n",
    "        \"left_ear\": 3,\n",
    "        \"right_ear\": 4,\n",
    "        \"left_shoulder\": 5,\n",
    "        \"right_shoulder\": 6,\n",
    "        \"left_elbow\": 7,\n",
    "        \"right_elbow\": 8,\n",
    "        \"left_wrist\": 9,\n",
    "        \"right_wrist\": 10,\n",
    "        \"left_hip\": 11,\n",
    "        \"right_hip\": 12,\n",
    "        \"left_knee\": 13,\n",
    "        \"right_knee\": 14,\n",
    "        \"left_ankle\": 15,\n",
    "        \"right_ankle\": 16}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only T2xx have a pose tracking sensor, so we cannot acces pose frames with the d455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = rs.pipeline()\n",
    "config = rs.config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyrealsense2.device: Intel RealSense D455 (S/N: 141322250607  FW: 05.14.00.00  on USB3.2)>\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the devie has the following sensors: \n",
      "Stereo Module\n",
      "RGB Camera\n",
      "Motion Module\n"
     ]
    }
   ],
   "source": [
    "print(\"the devie has the following sensors: \")\n",
    "for s in device.sensors:\n",
    "    print(s.get_info(rs.camera_info.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setups streams of data that will be sent by the camera \n",
    "\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "if device_product_line == 'L500':\n",
    "    config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)\n",
    "else:\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "config.enable_stream(rs.stream.gyro)\n",
    "config.enable_stream(rs.stream.accel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix_from_vectors(vec1, vec2):\n",
    "    \"\"\" Find the rotation matrix that aligns vec1 to vec2\n",
    "    :param vec1: A 3d \"source\" vector\n",
    "    :param vec2: A 3d \"destination\" vector\n",
    "    :return mat: A transform matrix (3x3) which when applied to vec1, aligns it with vec2.\n",
    "    \"\"\"\n",
    "    a, b = (vec1 / np.linalg.norm(vec1)).reshape(3), (vec2 / np.linalg.norm(vec2)).reshape(3)\n",
    "    v = np.cross(a, b)\n",
    "    c = np.dot(a, b)\n",
    "    s = np.linalg.norm(v)\n",
    "    kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n",
    "    rotation_matrix = np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2))\n",
    "    return rotation_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be used when pipeline.stop() was not called (due to a crash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming\n",
    "#pipeline.stop()\n",
    "cfg = pipeline.start(config)\n",
    "\n",
    "BASE_ORIENTATION = [0, 0, -1]\n",
    "\n",
    "detection = True\n",
    "\n",
    "colorizer = rs.colorizer() # to colorize depth images\n",
    "\n",
    "profile = cfg.get_stream(rs.stream.depth) # Fetch stream profile for depth stream\n",
    "intr = profile.as_video_stream_profile().get_intrinsics() # Downcast to video_stream_profile and fetch intrinsics\n",
    "\n",
    "while True:\n",
    "\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27: # press escape to quit\n",
    "        break\n",
    "\n",
    "    # Wait for a coherent pair of frames: depth and color\n",
    "    frames = pipeline.wait_for_frames() # depth, rgb, accel, gyro\n",
    "\n",
    "    accel_frame = frames[2]\n",
    "    gyro_frame = frames[3]\n",
    "   \n",
    "    if gyro_frame:\n",
    "        gyro_data = gyro_frame.as_motion_frame().get_motion_data()\n",
    "\n",
    "    if accel_frame:\n",
    "        accel_data = accel_frame.as_motion_frame().get_motion_data()\n",
    "        orientation = [accel_data.x, accel_data.y, accel_data.z]\n",
    "        orientation[1], orientation[2] = orientation[2], orientation[1] #swap y and z\n",
    "        rotation = rotation_matrix_from_vectors(orientation, BASE_ORIENTATION) # rotation required to be aligned with gravity\n",
    "\n",
    "    # Create alignment primitive with color as its target stream:\n",
    "    align = rs.align(rs.stream.color)\n",
    "    frameset = align.process(frames)\n",
    "\n",
    "    # get aligned frames\n",
    "    depth_frame = frameset.get_depth_frame()\n",
    "    color_frame = frameset.get_color_frame()\n",
    "    if not depth_frame or not color_frame:\n",
    "        continue\n",
    "\n",
    "    # Convert images to numpy arrays\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    \n",
    "    # ==== OPENPIFPAF =====\n",
    "    if detection:\n",
    "        # img = cv2.cvtColor(color_frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = PIL.Image.fromarray(color_image)\n",
    "        frame = color_image\n",
    "\n",
    "        predictions, _, _ = predictor.pil_image(pil_img)\n",
    "\n",
    "        try:\n",
    "           \n",
    "            # ==== skeleton drawing ====\n",
    "            # keypoints\n",
    "            keypoints = predictions[0].data\n",
    "\n",
    "            # left arm\n",
    "            left_shoulder = keypoints[pos_dict[\"left_shoulder\"],0:2]\n",
    "            left_hip = keypoints[pos_dict[\"left_hip\"],0:2]\n",
    "            left_elbow = keypoints[pos_dict[\"left_elbow\"],0:2]\n",
    "            left_wrist = keypoints[pos_dict[\"left_wrist\"],0:2]\n",
    "\n",
    "            # right arm\n",
    "            right_shoulder = keypoints[pos_dict[\"right_shoulder\"],0:2]\n",
    "            right_hip = keypoints[pos_dict[\"right_hip\"],0:2]\n",
    "            right_elbow = keypoints[pos_dict[\"right_elbow\"],0:2]\n",
    "            right_wrist = keypoints[pos_dict[\"right_wrist\"],0:2]\n",
    "\n",
    "            # draw only right forearm\n",
    "            cv2.line(frame, tuple(map(int, tuple(right_elbow))), tuple(map(int, tuple(right_wrist))), RED, 5)\n",
    "            # display_whole_skeleton(frame, keypoints)\n",
    "            # ==== END skeleton drawing ====\n",
    "\n",
    "            # get 3d coordinates of the left and right elbow\n",
    "            left_elbow_3d = rs.rs2_deproject_pixel_to_point(intr, left_elbow, depth_frame.get_distance(int(left_elbow[0]), int(left_elbow[1])))\n",
    "            right_elbow_3d = rs.rs2_deproject_pixel_to_point(intr, right_elbow, depth_frame.get_distance(int(right_elbow[0]), int(right_elbow[1])))\n",
    "\n",
    "            #get 3d coordinates of the left and right wrists\n",
    "            left_wrist_3d = rs.rs2_deproject_pixel_to_point(intr, left_wrist, depth_frame.get_distance(int(left_wrist[0]), int(left_wrist[1])))\n",
    "            right_wrist_3d = rs.rs2_deproject_pixel_to_point(intr, right_wrist, depth_frame.get_distance(int(right_wrist[0]), int(right_wrist[1])))\n",
    "\n",
    "            # compute vector from elbow to wrist\n",
    "            left_forearm_vector = np.array(left_wrist_3d) - np.array(left_elbow_3d)\n",
    "            right_forearm_vector = np.array(right_wrist_3d) - np.array(right_elbow_3d)\n",
    "\n",
    "            right_forearm_vector[[1,2]] = right_forearm_vector[[2,1]] # swap y and z\n",
    "            # right_forearm_vector[2] = -right_forearm_vector[2] # invert z\n",
    "\n",
    "            corrected_forearm_vector = rotation.dot(right_forearm_vector) # rotate forearm vector to align with gravity\n",
    "            corrected_forearm_vector[2] = -corrected_forearm_vector[2] # invert z to have positive elevation when arm is raised\n",
    "\n",
    "            # compute length of the forearm\n",
    "            left_forearm_length = np.linalg.norm(left_forearm_vector)\n",
    "            right_forearm_length = np.linalg.norm(right_forearm_vector)\n",
    "\n",
    "            # convert to spherical coordinates\n",
    "            _, elevation, azimuth = cart2sph(*corrected_forearm_vector)\n",
    "            elevation = rad2deg(elevation)\n",
    "            azimuth = rad2deg(azimuth)\n",
    "\n",
    "            cv2.putText(img = frame, text=f\"elevation: {elevation:.1f}, azimuth: {azimuth:.1f}\", org = (0,60), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=0.5, color=GREEN,thickness=1)\n",
    "            cv2.putText(img = frame, text=f\"right forearm vector: {right_forearm_vector}\", org = (0,120), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=0.5, color=GREEN,thickness=1)\n",
    "            cv2.putText(img = frame, text=f\"corrected forearm vector: {corrected_forearm_vector}\", org = (0,180), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=0.5, color=GREEN,thickness=1)\n",
    "\n",
    "            # print(f\"elevation: {rad2deg(elevation)}, azimuth: {rad2deg(azimuth)}\", end = \"\\r\")\n",
    "            # print(f\"right forearm vector: {right_forearm_vector}\", end = \"\\r\")\n",
    "\n",
    "        except:\n",
    "            cv2.putText(img = frame, text=f\"no detection\", org = (0,60), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=GREEN,thickness=2)\n",
    "\n",
    "    # ==== END OPENPIFPAF =====\n",
    "    \n",
    "    # Update color and depth frames:\n",
    "    aligned_depth_frame = frameset.get_depth_frame()\n",
    "    colorized_depth = np.asanyarray(colorizer.colorize(aligned_depth_frame).get_data())\n",
    "\n",
    "    # Show the two frames together:\n",
    "    images = np.hstack((color_image, colorized_depth))\n",
    "    # plt.imshow(images)\n",
    "\n",
    "    # Show images\n",
    "    cv2.namedWindow('RealSense', cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('RealSense', images)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# Stop streaming\n",
    "pipeline.stop()\n",
    "\n",
    "# Release resources\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.deproject_pixel_to_point()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:16:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a1eca162d8244ec663334057c1610a3bae01391a28d85af84dac413dee83203"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
