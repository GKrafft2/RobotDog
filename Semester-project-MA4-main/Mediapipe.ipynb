{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 17:26:46.489977: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 17:26:47.629014: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/guillaume/anaconda3/envs/semproj/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.7/lib64:\n",
      "2023-06-02 17:26:47.629084: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/guillaume/anaconda3/envs/semproj/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.7/lib64:\n",
      "2023-06-02 17:26:47.629092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/guillaume/anaconda3/envs/semproj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import openpifpaf\n",
    "import PIL\n",
    "import torch\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "BG_COLOR = (192, 192, 192) # gray\n",
    "with mp_holistic.Holistic(\n",
    "    static_image_mode=True,\n",
    "    model_complexity=2,\n",
    "    enable_segmentation=True,\n",
    "    refine_face_landmarks=True) as holistic:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    image = cv2.imread(file)\n",
    "    image_height, image_width, _ = image.shape\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "      print(\n",
    "          f'Nose coordinates: ('\n",
    "          f'{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].x * image_width}, '\n",
    "          f'{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].y * image_height})'\n",
    "      )\n",
    "\n",
    "    annotated_image = image.copy()\n",
    "    # Draw segmentation on the image.\n",
    "    # To improve segmentation around boundaries, consider applying a joint\n",
    "    # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "    bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "    bg_image[:] = BG_COLOR\n",
    "    annotated_image = np.where(condition, annotated_image, bg_image)\n",
    "    # Draw pose, left and right hands, and face landmarks on the image.\n",
    "    mp_drawing.draw_landmarks(\n",
    "        annotated_image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        annotated_image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.\n",
    "        get_default_pose_landmarks_style())\n",
    "    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n",
    "    # Plot pose world landmarks.\n",
    "    mp_drawing.plot_landmarks(\n",
    "        results.pose_world_landmarks, mp_holistic.POSE_CONNECTIONS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = openpifpaf.Predictor(checkpoint='shufflenetv2k16')\n",
    "\n",
    "pos_dict = {\n",
    "        \"nose\": 0,\n",
    "        \"left_eye\": 1,\n",
    "        \"right_eye\": 2,\n",
    "        \"left_ear\": 3,\n",
    "        \"right_ear\": 4,\n",
    "        \"left_shoudler\": 5,\n",
    "        \"right_shoudler\": 6,\n",
    "        \"left_elbow\": 7,\n",
    "        \"right_elbow\": 8,\n",
    "        \"left_wrist\": 9,\n",
    "        \"right_wrist\": 10,\n",
    "        \"left_hip\": 11,\n",
    "        \"right_hip\": 12,\n",
    "        \"left_knee\": 13,\n",
    "        \"right_knee\": 14,\n",
    "        \"left_ankle\": 15,\n",
    "        \"right_ankle\": 16}\n",
    "\n",
    "RED = (0, 0, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (255, 0, 0)\n",
    "YELLOW = (0, 255, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPF = False\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture()\n",
    "with mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    "  while cap.isOpened():\n",
    "    start = time()\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'coÂ£ntinue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image)\n",
    "\n",
    "    if OPPF:\n",
    "        #OpenPifPaf\n",
    "        pil_img = PIL.Image.fromarray(image)\n",
    "        predictions, _, _ = predictor.pil_image(pil_img)\n",
    "\n",
    "        try:\n",
    "            \n",
    "            #draw arms and hips\n",
    "            keypoints = predictions[0].data\n",
    "\n",
    "            # left arm\n",
    "            left_shoulder = keypoints[pos_dict[\"left_shoudler\"],0:2]\n",
    "            left_hip = keypoints[pos_dict[\"left_hip\"],0:2]\n",
    "            left_elbow = keypoints[pos_dict[\"left_elbow\"],0:2]\n",
    "\n",
    "            # right arm\n",
    "            right_shoulder = keypoints[pos_dict[\"right_shoudler\"],0:2]\n",
    "            right_hip = keypoints[pos_dict[\"right_hip\"],0:2]\n",
    "            right_elbow = keypoints[pos_dict[\"right_elbow\"],0:2]\n",
    "\n",
    "            cv2.line(image, tuple(map(int, tuple(left_shoulder))), tuple(map(int, tuple(left_elbow))), RED, 5)\n",
    "            cv2.line(image, tuple(map(int, tuple(left_shoulder))), tuple(map(int, tuple(left_hip))), YELLOW, 5)\n",
    "            cv2.line(image, tuple(map(int, tuple(right_shoulder))), tuple(map(int, tuple(right_elbow))), RED, 5)\n",
    "            cv2.line(image, tuple(map(int, tuple(right_shoulder))), tuple(map(int, tuple(right_hip))), YELLOW, 5)\n",
    "        except:\n",
    "            cv2.putText(img = image, text=f\"no detection\", org = (0,60), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=GREEN,thickness=2)\n",
    "\n",
    "    # Draw landmark annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27: # press escape to quit\n",
    "        break\n",
    "    stop = time()\n",
    "\n",
    "    # writes fps\n",
    "    fps = 1/(stop-start)\n",
    "    cv2.putText(img = image, text=f\"{fps:.2f} fps\", org = (0,30), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=5, color=GREEN,thickness=2)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@206.808] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@206.926] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time \n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "new_frame_time = 0\n",
    "prev_frame_time = 0\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as pose:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    new_frame_time = time.time()\n",
    "    fps = 1/(new_frame_time-prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    print(image.shape)\n",
    "    results = pose.process(image)\n",
    "\n",
    "    # Draw the pose annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    fps = str(int(fps))\n",
    "    cv2.putText(image, fps, (7, 70), 1, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "    cv2.imshow('MediaPipe Pose', cv2.flip(image, 1))\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27: # press escape to quit\n",
    "        break\n",
    "cap.release()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - nose\n",
    "1 - left eye (inner)\n",
    "2 - left eye\n",
    "3 - left eye (outer)\n",
    "4 - right eye (inner)\n",
    "5 - right eye\n",
    "6 - right eye (outer)\n",
    "7 - left ear\n",
    "8 - right ear\n",
    "9 - mouth (left)\n",
    "10 - mouth (right)\n",
    "11 - left shoulder\n",
    "12 - right shoulder\n",
    "13 - left elbow\n",
    "14 - right elbow\n",
    "15 - left wrist\n",
    "16 - right wrist\n",
    "17 - left pinky\n",
    "18 - right pinky\n",
    "19 - left index\n",
    "20 - right index\n",
    "21 - left thumb\n",
    "22 - right thumb\n",
    "23 - left hip\n",
    "24 - right hip\n",
    "25 - left knee\n",
    "26 - right knee\n",
    "27 - left ankle\n",
    "28 - right ankle\n",
    "29 - left heel\n",
    "30 - right heel\n",
    "31 - left foot index\n",
    "32 - right foot index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_index = {\"nose\":0,\n",
    "\"left eye (inner)\":1,\n",
    "\"left eye\":2,\n",
    "\"left eye (outer)\":3,\n",
    "\"right eye (inner)\":4,\n",
    "\"right eye\":5,\n",
    "\"right eye (outer)\":6,\n",
    "\"left ear\":7,\n",
    "\"right ear\":8,\n",
    "\"mouth (left)\":9,\n",
    "\"mouth (right)\":10,\n",
    "\"left shoulder\":11,\n",
    "\"right shoulder\":12,\n",
    "\"left elbow\":13,\n",
    "\"right elbow\":14,\n",
    "\"left wrist\":15,\n",
    "\"right wrist\":16,\n",
    "\"left pinky\":17,\n",
    "\"right pinky\":18,\n",
    "\"left index\":19,\n",
    "\"right index\":20,\n",
    "\"left thumb\":21,\n",
    "\"right thumb\":22,\n",
    "\"left hip\":23,\n",
    "\"right hip\":24,\n",
    "\"left knee\":25,\n",
    "\"right knee\":26,\n",
    "\"left ankle\":27,\n",
    "\"right ankle\":28,\n",
    "\"left heel\":29,\n",
    "\"right heel\":30,\n",
    "\"left foot index\":31,\n",
    "\"right foot index\":32}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = [480,640]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.6065418720245361\n",
       "y: 0.5476277470588684\n",
       "z: -2.819145917892456\n",
       "visibility: 0.998938798904419"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x: 0.6065418720245361\n",
       "y: 0.5476277470588684\n",
       "z: -2.819145917892456\n",
       "visibility: 0.998938798904419\n",
       ", x: 0.6495431661605835\n",
       "y: 0.4248652160167694\n",
       "z: -2.751957416534424\n",
       "visibility: 0.9977502226829529\n",
       ", x: 0.6757246255874634\n",
       "y: 0.42076441645622253\n",
       "z: -2.752040147781372\n",
       "visibility: 0.9980242252349854\n",
       ", x: 0.702030599117279\n",
       "y: 0.4170161187648773\n",
       "z: -2.7525336742401123\n",
       "visibility: 0.9970047473907471\n",
       ", x: 0.5584686398506165\n",
       "y: 0.4247419238090515\n",
       "z: -2.774761199951172\n",
       "visibility: 0.9981383681297302\n",
       ", x: 0.5218402743339539\n",
       "y: 0.4202928841114044\n",
       "z: -2.7738561630249023\n",
       "visibility: 0.9985030889511108\n",
       ", x: 0.48931750655174255\n",
       "y: 0.4157469570636749\n",
       "z: -2.7744686603546143\n",
       "visibility: 0.9982019662857056\n",
       ", x: 0.7390888929367065\n",
       "y: 0.4269128441810608\n",
       "z: -1.9621719121932983\n",
       "visibility: 0.997580349445343\n",
       ", x: 0.4308583736419678\n",
       "y: 0.42057880759239197\n",
       "z: -2.0330443382263184\n",
       "visibility: 0.9991651773452759\n",
       ", x: 0.6449480056762695\n",
       "y: 0.6446468830108643\n",
       "z: -2.487389087677002\n",
       "visibility: 0.9988939762115479\n",
       ", x: 0.5398501753807068\n",
       "y: 0.6621905565261841\n",
       "z: -2.5097768306732178\n",
       "visibility: 0.9993338584899902\n",
       ", x: 0.9286333918571472\n",
       "y: 0.8770209550857544\n",
       "z: -1.2044230699539185\n",
       "visibility: 0.9928087592124939\n",
       ", x: 0.19429180026054382\n",
       "y: 0.8525000810623169\n",
       "z: -1.3532238006591797\n",
       "visibility: 0.995982825756073\n",
       ", x: 1.1335561275482178\n",
       "y: 1.35903799533844\n",
       "z: -1.1335505247116089\n",
       "visibility: 0.08943834155797958\n",
       ", x: -0.05122266337275505\n",
       "y: 1.4618897438049316\n",
       "z: -1.1486090421676636\n",
       "visibility: 0.4716855585575104\n",
       ", x: 1.114419937133789\n",
       "y: 1.8453247547149658\n",
       "z: -2.0124902725219727\n",
       "visibility: 0.014603574760258198\n",
       ", x: 0.006550136022269726\n",
       "y: 1.9832649230957031\n",
       "z: -1.8150078058242798\n",
       "visibility: 0.06348996609449387\n",
       ", x: 1.15244722366333\n",
       "y: 2.0125486850738525\n",
       "z: -2.251070499420166\n",
       "visibility: 0.017141925171017647\n",
       ", x: -0.0058701843954622746\n",
       "y: 2.172137498855591\n",
       "z: -2.0463449954986572\n",
       "visibility: 0.056263260543346405\n",
       ", x: 1.0877134799957275\n",
       "y: 2.0070714950561523\n",
       "z: -2.371382713317871\n",
       "visibility: 0.027791161090135574\n",
       ", x: 0.06759357452392578\n",
       "y: 2.138662099838257\n",
       "z: -2.1971330642700195\n",
       "visibility: 0.090351402759552\n",
       ", x: 1.052550196647644\n",
       "y: 1.948705792427063\n",
       "z: -2.102532148361206\n",
       "visibility: 0.027464531362056732\n",
       ", x: 0.08778003603219986\n",
       "y: 2.0710957050323486\n",
       "z: -1.8979896306991577\n",
       "visibility: 0.08077672868967056\n",
       ", x: 0.7802897691726685\n",
       "y: 1.9712570905685425\n",
       "z: -0.11008359491825104\n",
       "visibility: 0.0002815839252434671\n",
       ", x: 0.3038918673992157\n",
       "y: 1.9716688394546509\n",
       "z: 0.11888915300369263\n",
       "visibility: 0.0002601702872198075\n",
       ", x: 0.7853289842605591\n",
       "y: 2.936690092086792\n",
       "z: -0.15047615766525269\n",
       "visibility: 0.0007031672284938395\n",
       ", x: 0.35965871810913086\n",
       "y: 2.9265999794006348\n",
       "z: 0.15833085775375366\n",
       "visibility: 0.000291245523840189\n",
       ", x: 0.79208904504776\n",
       "y: 3.7811715602874756\n",
       "z: 0.9426838159561157\n",
       "visibility: 6.330979522317648e-05\n",
       ", x: 0.3901323974132538\n",
       "y: 3.765784502029419\n",
       "z: 1.0597174167633057\n",
       "visibility: 7.5212769843346905e-06\n",
       ", x: 0.8040391802787781\n",
       "y: 3.9181694984436035\n",
       "z: 0.9840294122695923\n",
       "visibility: 4.2072879296028987e-05\n",
       ", x: 0.38138967752456665\n",
       "y: 3.9039273262023926\n",
       "z: 1.1147643327713013\n",
       "visibility: 2.5330460630357265e-05\n",
       ", x: 0.7072908878326416\n",
       "y: 4.081820487976074\n",
       "z: -0.14699029922485352\n",
       "visibility: 4.135036942898296e-05\n",
       ", x: 0.4749278128147125\n",
       "y: 4.068417549133301\n",
       "z: -0.0726567953824997\n",
       "visibility: 4.95301210321486e-05\n",
       "]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a1eca162d8244ec663334057c1610a3bae01391a28d85af84dac413dee83203"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
